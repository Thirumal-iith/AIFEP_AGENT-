# # GridWorldEnv (2x2) in YAML format

# A:
#   - [[[1.0, 0.0, 0.0, 0.0],
#       [0.0, 1.0, 0.0, 0.0],
#       [0.0, 0.0, 1.0, 0.0],
#       [0.0, 0.0, 0.0, 1.0]]]

# B:
#   - [[[0.0, 1.0, 0.0, 0.0],    # UP action (stay if at top row)
#       [0.0, 1.0, 0.0, 0.0],
#       [1.0, 0.0, 0.0, 0.0],
#       [0.0, 0.0, 1.0, 0.0]],

#      [[0.0, 0.0, 0.0, 0.0],    # RIGHT action
#       [0.0, 1.0, 0.0, 0.0],
#       [0.0, 0.0, 0.0, 0.0],
#       [0.0, 0.0, 1.0, 0.0]],

#      [[1.0, 0.0, 0.0, 0.0],    # DOWN action
#       [0.0, 0.0, 1.0, 0.0],
#       [0.0, 0.0, 1.0, 0.0],
#       [0.0, 0.0, 0.0, 1.0]],

#      [[1.0, 0.0, 0.0, 0.0],    # LEFT action
#       [0.0, 1.0, 0.0, 0.0],
#       [0.0, 0.0, 0.0, 1.0],
#       [0.0, 0.0, 0.0, 1.0]],

#      [[1.0, 0.0, 0.0, 0.0],    # STAY action
#       [0.0, 1.0, 0.0, 0.0],
#       [0.0, 0.0, 1.0, 0.0],
#       [0.0, 0.0, 0.0, 1.0]]]]

# C:
#   - [0.0, 0.0, 0.0, 0.0]   # No preferences over observations

# D:
#   - [1.0, 0.0, 0.0, 0.0]   # Prior: start at state 0 (top-left)

# E: null

# pA:
#   - [[[1.0, 1.0, 1.0, 1.0],
#       [1.0, 1.0, 1.0, 1.0],
#       [1.0, 1.0, 1.0, 1.0],
#       [1.0, 1.0, 1.0, 1.0]]]

# pB:
#   - [[[1.0, 1.0, 1.0, 1.0],
#       [1.0, 1.0, 1.0, 1.0],
#       [1.0, 1.0, 1.0, 1.0],
#       [1.0, 1.0, 1.0, 1.0]],
#      [[1.0, 1.0, 1.0, 1.0],
#       [1.0, 1.0, 1.0, 1.0],
#       [1.0, 1.0, 1.0, 1.0],
#       [1.0, 1.0, 1.0, 1.0]],
#      [[1.0, 1.0, 1.0, 1.0],
#       [1.0, 1.0, 1.0, 1.0],
#       [1.0, 1.0, 1.0, 1.0],
#       [1.0, 1.0, 1.0, 1.0]],
#      [[1.0, 1.0, 1.0, 1.0],
#       [1.0, 1.0, 1.0, 1.0],
#       [1.0, 1.0, 1.0, 1.0],
#       [1.0, 1.0, 1.0, 1.0]],
#      [[1.0, 1.0, 1.0, 1.0],
#       [1.0, 1.0, 1.0, 1.0],
#       [1.0, 1.0, 1.0, 1.0],
#       [1.0, 1.0, 1.0, 1.0]]]]

# pD:
#   - [1.0, 1.0, 1.0, 1.0]

# A_factor_list: [[0]]     # Observations depend on hidden state
# B_factor_list: [[0]]
# T: 5
# initial_observations:
#   - [0, 1, 2, 3, 0]     # Example trajectory of observed states
# num_controls: [5]
# policy_len: 1
# inference_horizon: 1
# control_fac_idx: null
# policies: null
# gamma: 16.0
# alpha: 16.0
# use_utility: 1
# use_states_info_gain: 1
# use_param_info_gain: 0
# action_selection: deterministic
# sampling_mode: marginal
# inference_algo: VANILLA
# inference_params: null
# modalities_to_learn: all
# lr_pA: 1.0
# factors_to_learn: all
# lr_pB: 1.0
# lr_pD: 1.0

# A matrix: observation model - columns should sum to 1
# For a grid world where observations directly correspond to states (perfect observation)
# Shape should be (num_observations, num_states)
A: 
   - [[[1.0, 0.0, 0.0, 0.0],
     [0.0, 1.0, 0.0, 0.0],
     [0.0, 0.0, 1.0, 0.0],
     [0.0, 0.0, 0.0, 1.0]]] 

# B matrix: transition model - each action's matrix columns should sum to 1
# Shape: (num_actions, num_states, num_states)
B: 
  - [[[1.0, 0.0, 1.0, 0.0],    # UP action (state transitions)
     [0.0, 1.0, 0.0, 1.0],    # from each state (columns) to next state (rows)
     [0.0, 0.0, 0.0, 0.0],
     [0.0, 0.0, 0.0, 0.0]],

    [[0.0, 1.0, 0.0, 0.0],    # RIGHT action
     [0.0, 0.0, 0.0, 1.0],
     [1.0, 0.0, 1.0, 0.0],
     [0.0, 0.0, 0.0, 0.0]],

    [[0.0, 0.0, 0.0, 0.0],    # DOWN action
     [0.0, 0.0, 0.0, 0.0],
     [1.0, 0.0, 1.0, 0.0],
     [0.0, 1.0, 0.0, 1.0]],

    [[0.0, 0.0, 0.0, 0.0],    # LEFT action
     [1.0, 0.0, 0.0, 0.0],
     [0.0, 0.0, 1.0, 0.0],
     [0.0, 1.0, 0.0, 1.0]],

    [[1.0, 0.0, 0.0, 0.0],    # STAY action (identity)
     [0.0, 1.0, 0.0, 0.0],
     [0.0, 0.0, 1.0, 0.0],
     [0.0, 0.0, 0.0, 1.0]]]

# C matrix: preferences (no preference for any observation)
C: - [0.0, 0.0, 0.0, 0.0]

# D matrix: initial state belief (start at state 0)
D: - [1.0, 0.0, 0.0, 0.0]

E: null

# Prior beliefs about A matrix (uniform)
pA: 
  - [[[1.0, 1.0, 1.0, 1.0],
      [1.0, 1.0, 1.0, 1.0],
      [1.0, 1.0, 1.0, 1.0],
      [1.0, 1.0, 1.0, 1.0]]]

# Prior beliefs about B matrix (uniform)
pB: 
    - [[[1.0, 1.0, 1.0, 1.0],
      [1.0, 1.0, 1.0, 1.0],
      [1.0, 1.0, 1.0, 1.0],
      [1.0, 1.0, 1.0, 1.0]],
     [[1.0, 1.0, 1.0, 1.0],
      [1.0, 1.0, 1.0, 1.0],
      [1.0, 1.0, 1.0, 1.0],
      [1.0, 1.0, 1.0, 1.0]],
     [[1.0, 1.0, 1.0, 1.0],
      [1.0, 1.0, 1.0, 1.0],
      [1.0, 1.0, 1.0, 1.0],
      [1.0, 1.0, 1.0, 1.0]],
     [[1.0, 1.0, 1.0, 1.0],
      [1.0, 1.0, 1.0, 1.0],
      [1.0, 1.0, 1.0, 1.0],
      [1.0, 1.0, 1.0, 1.0]],
     [[1.0, 1.0, 1.0, 1.0],
      [1.0, 1.0, 1.0, 1.0],
      [1.0, 1.0, 1.0, 1.0],
      [1.0, 1.0, 1.0, 1.0]]]

# Prior beliefs about D matrix
pD: 
  - [[1.0, 1.0, 1.0, 1.0]]

# Configuration parameters
A_factor_list: [[0]]
B_factor_list: [[0]]
T: 5
initial_observations:
  - [0, 1, 2, 3, 0]
num_controls: [5]
policy_len: 1
inference_horizon: 1
control_fac_idx: null
policies: null
gamma: 16.0
alpha: 16.0
use_utility: 1
use_states_info_gain: 1
use_param_info_gain: 0
action_selection: deterministic
sampling_mode: marginal
inference_algo: VANILLA
inference_params: null
modalities_to_learn: all
lr_pA: 1.0
factors_to_learn: all
lr_pB: 1.0
lr_pD: 1.0